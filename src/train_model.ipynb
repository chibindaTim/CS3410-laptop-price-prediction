{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a476ae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation (Evaluation of model)\n",
    "def cross_validate(X , y, alphas , cv ): #training dataset features, target , regularization strength, buckets/folds\n",
    "\n",
    "    #cross validate with different alpha\n",
    "    best_alpha = None\n",
    "    best_score = float('inf')\n",
    "    cv_results = {}\n",
    "   \n",
    "    for alpha in alphas:\n",
    "        fold_size = len(X) // cv\n",
    "        scores = []\n",
    "\n",
    "    #create validation/train set\n",
    "        for fold in range(cv):\n",
    "        #split the dataset into train and test sets\n",
    "            start = fold * fold_size #creates starting point for each fold\n",
    "            if fold < cv-1:\n",
    "                end=start + fold_size\n",
    "            else:\n",
    "                end=len(X)\n",
    "    #Create the validation dataset /test dataset\n",
    "            X_val =X[start:end]\n",
    "            y_val =y[start:end]\n",
    "\n",
    "    #Create training dataset , concatenate elements before start and after end\n",
    "            X_train = np.concatenate([X[:start],X[end:]])\n",
    "            y_train = np.concatenate([y[:start],y[end:]])\n",
    "\n",
    "    #Train model using lasso\n",
    "            LassoTrain= LassoRegression(alpha=alpha)\n",
    "            LassoTrain.fit(X_train,y_train)\n",
    "\n",
    "\n",
    "    #Predict and Evaluate the model\n",
    "            y_pred=LassoTrain.predict(X_val)\n",
    "            metrics = evaluate_model(y_val, y_pred)   # call the function\n",
    "            mse = metrics['mse']                      # then access 'mse' from the result\n",
    "    #returns regression metrics Regression Metrics:\n",
    "    #Mean Squared Error (MSE): <value>\n",
    "    #Root Mean Squared Error (RMSE): <value>\n",
    "    #R-squared (RÂ²) Score: <value>\n",
    "            scores.append(mse)\n",
    "\n",
    "        avg_score = np.mean(scores)\n",
    "        std_score = np.std(scores)\n",
    "\n",
    "        cv_results[alpha] = {\n",
    "            'mean_mse': avg_score,\n",
    "            'std_mse': std_score,\n",
    "            'scores': scores\n",
    "        }\n",
    "\n",
    "    #select the best alpha/ learning rate\n",
    "        if avg_score < best_score:\n",
    "            best_score = avg_score\n",
    "            best_alpha = alpha\n",
    "\n",
    "    return best_alpha, cv_results\n",
    "\n",
    "\n",
    "alphas = [0.001,0.0001,0.5,0.55 ,0.01, 0.1, 1.0, 10.0]\n",
    "\n",
    "#best_alpha, cv_results = cross_validate(X_scaled, y, alphas, 10)\n",
    "#print(\"Best alpha:\", best_alpha)\n",
    "#print(\"CV results:\", cv_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b94be28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define Regression Metrics to evaluate model\n",
    "def evaluate_model(y_val, y_pred):\n",
    "    mse = np.mean((y_val-y_pred)**2) #mean squared error\n",
    "    rmse = np.sqrt(mse) #root mean square error/validation error\n",
    "    #find r^2 score\n",
    "    #use formula: 1 - [(Residual Sum of Squares (RSS))/(Total Sum of Squares(TSS))]\n",
    "    rss = np.sum((y_val-y_pred)**2)\n",
    "    mean= np.mean(y_val)\n",
    "    tss = np.sum((y_val-mean)**2)\n",
    "    r_sqaured_score= 1-(rss/tss)\n",
    "\n",
    "    return {'mse': mse, 'rmse': rmse, 'r_sqaured_score': r_sqaured_score}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
