{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"none","dataSources":[],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Model 1 draft\n#L1 regularization\nimport numpy as np\nimport pickle\n\nclass LassoRegression:\n    def __init__(self, learning_rate=0.01, alpha=1.0,max_iter=1000 ): #alpha is the threshold strength and max_iter is number of times you can descend the slope\n        \n        self.learning_rate = learning_rate #intent to use gradient descent to minimize error in the linear regression model\n        self.feature_names = None\n        self.selected_features = {}\n        self.max_iter = max_iter\n        self.alpha = alpha \n        self.weights = None\n        self.bias = None\n        \n    def _soft_threshold(self, x, thresh): #thresh = learning_rate*regularization_strength which is alpha\n        # Handles the non-differentiability at 0\n        return np.sign(x) * np.maximum(np.abs(x) - thresh, 0) #if weight is small set to 0, if weight is large shrink towards 0\n\n    def fit(self, X, y):\n        n_samples, n_features = X.shape\n\n        # Initialize coefficients\n        self.weights = np.zeros(n_features)   # start with w=0 \n        self.bias = 0                         # intercept\n\n        # Gradient Descent Loop\n        for i in range(self.max_iter):\n            # Prediction get dot product of the features with weights and include self bias\n            #Formula Xw+b where X is a nx d matrix (n samples d features), and w is a dx1 matrix(d weights)\n            y_pred = np.dot(X, self.weights) + self.bias \n\n            #Compute Residual (errors), gradient of error\n            residuals = y_pred - y\n\n            #Compute gradients, which is partial derivatives , wrt to weights and bias respectively\n            dw = (1/n_samples) * np.dot(X.T, residuals) #\n            db = (1/n_samples) * np.sum(residuals) #\n\n            # Gradient descent update with shrinkage\n            self.weights = self._soft_threshold(\n                self.weights - self.learning_rate * dw,\n                self.alpha * self.learning_rate\n            )\n            self.bias -= self.learning_rate * db   # bias not penalized\n        return self   \n\n    def predict(self, X):\n        if self.weights is None:\n            raise ValueError(\"Model must be fitted before making predictions\")\n        return np.dot(X, self.weights) + self.bias\n\n    def get_params(self):\n        \"\"\"Return learned coefficients and intercept\"\"\"\n        return {\"weights\": self.weights, \"bias\": self.bias}\n\n\n    def get_selected_features(self, feature_names=None, threshold=1e-5):\n        if self.weights is None:\n            raise ValueError(\"Model must be fitted first\")\n        \n        selected_idx = np.abs(self.weights) > threshold\n        selected_weights = self.weights[selected_idx]\n        \n        if feature_names is not None:\n            selected_names = [feature_names[i] for i in np.where(selected_idx)[0]]\n            return dict(zip(selected_names, selected_weights))\n        else:\n            selected_indices = np.where(selected_idx)[0]\n            return dict(zip(selected_indices, selected_weights))\n\ndef save_model(model, filename):\n    #Save the trained model to a pickle file.\n\n    with open(filename, 'wb') as f:\n        pickle.dump(model, f)\n    print(f\"Model saved to {filename}\")\n\ndef load_model(filename):\n    #Load a trained model from a pickle file.\n\n    with open(filename, 'rb') as f:\n        model = pickle.load(f)\n    print(f\"Model loaded from {filename}\")\n    return model\n\n\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}