{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8186b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model 1 draft\n",
    "#L1 regularization\n",
    "import numpy as np\n",
    "\n",
    "class LassoRegression:\n",
    "    def __init__(self, learning_rate=0.01, alpha=1.0,max_iter=1000 ): #alpha is the threshold strength and max_iter is number of times you can descend the slope\n",
    "        \n",
    "        self.learning_rate = learning_rate #intent to use gradient descent to minimize error in the linear regression model\n",
    "        self.feature_names = None\n",
    "        self.selected_features = {}\n",
    "        self.max_iter = max_iter\n",
    "        self.alpha = alpha \n",
    "    def _soft_threshold(self, x, thresh): #thresh = learning_rate*regularization_strength which is alpha\n",
    "        # Handles the non-differentiability at 0\n",
    "        return np.sign(x) * np.maximum(np.abs(x) - thresh, 0) #if weight is small set to 0, if weight is large shrink towards 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize coefficients\n",
    "        self.weights = np.zeros(n_features)   # start with w=0 \n",
    "        self.bias = 0                         # intercept\n",
    "\n",
    "        # Gradient Descent Loop\n",
    "        for i in range(self.max_iter):\n",
    "            # Prediction get dot product of the features with weights and include self bias\n",
    "            #Formula Xw+b where X is a nx d matrix (n samples d features), and w is a dx1 matrix(d weights)\n",
    "            y_pred = np.dot(X, self.weights) + self.bias \n",
    "\n",
    "            #Compute Residual (errors) \n",
    "            residuals = y_pred - y\n",
    "\n",
    "            #Compute gradients, which is partial derivatives , wrt to weights and bias respectively\n",
    "            dw = (1/n_samples) * np.dot(X.T, residuals) #\n",
    "            db = (1/n_samples) * np.sum(residuals) #\n",
    "\n",
    "            # Gradient descent update with shrinkage\n",
    "            self.weights = self._soft_threshold(\n",
    "                self.weights - self.learning_rate * dw,\n",
    "                self.alpha * self.learning_rate\n",
    "            )\n",
    "            self.bias -= self.learning_rate * db   # bias not penalized\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"Return learned coefficients and intercept\"\"\"\n",
    "        return {\"weights\": self.weights, \"bias\": self.bias}\n",
    "\n",
    "\n",
    "# Separate features and target df_imputed_copy= df_imputed.copy() # Split features and target y = df_imputed_copy['Life expectancy'].astype(float).values X = df_imputed_copy.drop(columns=['Life expectancy','Country','Status']).astype(float).values lasso = LassoRegression(alpha=0.1, learning_rate=0.01, max_iter=1000) lasso.fit(X, y) print(lasso.get_params())\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
