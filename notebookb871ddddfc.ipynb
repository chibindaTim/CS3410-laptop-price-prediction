{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83c02e36",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-13T05:39:37.619661Z",
     "iopub.status.busy": "2025-09-13T05:39:37.619261Z",
     "iopub.status.idle": "2025-09-13T05:39:37.636330Z",
     "shell.execute_reply": "2025-09-13T05:39:37.635472Z"
    },
    "papermill": {
     "duration": 0.022229,
     "end_time": "2025-09-13T05:39:37.637873",
     "exception": false,
     "start_time": "2025-09-13T05:39:37.615644",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Model 1 draft\n",
    "#L1 regularization\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "class LassoRegression:\n",
    "    def __init__(self, learning_rate=0.01, alpha=1.0,max_iter=1000 ): #alpha is the threshold strength and max_iter is number of times you can descend the slope\n",
    "        \n",
    "        self.learning_rate = learning_rate #intent to use gradient descent to minimize error in the linear regression model\n",
    "        self.feature_names = None\n",
    "        self.selected_features = {}\n",
    "        self.max_iter = max_iter\n",
    "        self.alpha = alpha \n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        \n",
    "    def _soft_threshold(self, x, thresh): #thresh = learning_rate*regularization_strength which is alpha\n",
    "        # Handles the non-differentiability at 0\n",
    "        return np.sign(x) * np.maximum(np.abs(x) - thresh, 0) #if weight is small set to 0, if weight is large shrink towards 0\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Initialize coefficients\n",
    "        self.weights = np.zeros(n_features)   # start with w=0 \n",
    "        self.bias = 0                         # intercept\n",
    "\n",
    "        # Gradient Descent Loop\n",
    "        for i in range(self.max_iter):\n",
    "            # Prediction get dot product of the features with weights and include self bias\n",
    "            #Formula Xw+b where X is a nx d matrix (n samples d features), and w is a dx1 matrix(d weights)\n",
    "            y_pred = np.dot(X, self.weights) + self.bias \n",
    "\n",
    "            #Compute Residual (errors), gradient of error\n",
    "            residuals = y_pred - y\n",
    "\n",
    "            #Compute gradients, which is partial derivatives , wrt to weights and bias respectively\n",
    "            dw = (1/n_samples) * np.dot(X.T, residuals) #\n",
    "            db = (1/n_samples) * np.sum(residuals) #\n",
    "\n",
    "            # Gradient descent update with shrinkage\n",
    "            self.weights = self._soft_threshold(\n",
    "                self.weights - self.learning_rate * dw,\n",
    "                self.alpha * self.learning_rate\n",
    "            )\n",
    "            self.bias -= self.learning_rate * db   # bias not penalized\n",
    "        return self   \n",
    "\n",
    "    def predict(self, X):\n",
    "        if self.weights is None:\n",
    "            raise ValueError(\"Model must be fitted before making predictions\")\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "    def get_params(self):\n",
    "        \"\"\"Return learned coefficients and intercept\"\"\"\n",
    "        return {\"weights\": self.weights, \"bias\": self.bias}\n",
    "\n",
    "\n",
    "    def get_selected_features(self, feature_names=None, threshold=1e-5):\n",
    "        if self.weights is None:\n",
    "            raise ValueError(\"Model must be fitted first\")\n",
    "        \n",
    "        selected_idx = np.abs(self.weights) > threshold\n",
    "        selected_weights = self.weights[selected_idx]\n",
    "        \n",
    "        if feature_names is not None:\n",
    "            selected_names = [feature_names[i] for i in np.where(selected_idx)[0]]\n",
    "            return dict(zip(selected_names, selected_weights))\n",
    "        else:\n",
    "            selected_indices = np.where(selected_idx)[0]\n",
    "            return dict(zip(selected_indices, selected_weights))\n",
    "\n",
    "def save_model(model, filename):\n",
    "    #Save the trained model to a pickle file.\n",
    "\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    print(f\"Model saved to {filename}\")\n",
    "\n",
    "def load_model(filename):\n",
    "    #Load a trained model from a pickle file.\n",
    "\n",
    "    with open(filename, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    print(f\"Model loaded from {filename}\")\n",
    "    return model\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 6.019818,
   "end_time": "2025-09-13T05:39:38.058146",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-09-13T05:39:32.038328",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
